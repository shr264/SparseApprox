\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {paragraph}{Introduction:}{1}{section*.1}}
\newlabel{eq:lasso}{{1}{1}{Introduction:}{equation.0.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Subgradent Methods:}{1}{section*.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Subgradeint Algorithm with fixed step size\relax }}{2}{algorithm.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Subgradeint Algorithm with diminishing step size\relax }}{3}{algorithm.2}}
\@writefile{toc}{\contentsline {paragraph}{ISTA/FISTA}{4}{section*.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces ISTA with fixed step size\relax }}{5}{algorithm.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces ISTA with diminishing step size\relax }}{6}{algorithm.4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces FISTA with fixed step size\relax }}{7}{algorithm.5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces FISTA with diminishing step size\relax }}{8}{algorithm.6}}
\@writefile{toc}{\contentsline {paragraph}{ADMM}{10}{section*.4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces ADMM\relax }}{11}{algorithm.7}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Experiments for BP:}{12}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Experiments for GGM:}{12}{section*.6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Table showing the true positive and false positive rates for all methods. While ISTA and FISTA have the highest true recovery rate, they also have a higher error rate. The subgradient method recovers a very dense graph, while ADMM recovers a very sparse one.\relax }}{13}{table.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tprfpr}{{1}{13}{Table showing the true positive and false positive rates for all methods. While ISTA and FISTA have the highest true recovery rate, they also have a higher error rate. The subgradient method recovers a very dense graph, while ADMM recovers a very sparse one.\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Conclusion:}{13}{section*.8}}
\newlabel{fig:20}{{1a}{14}{$n=20$\relax }{figure.caption.9}{}}
\newlabel{sub@fig:20}{{a}{14}{$n=20$\relax }{figure.caption.9}{}}
\newlabel{fig:50}{{1b}{14}{$n=50$\relax }{figure.caption.9}{}}
\newlabel{sub@fig:50}{{b}{14}{$n=50$\relax }{figure.caption.9}{}}
\newlabel{fig:100}{{1c}{14}{$n=100$\relax }{figure.caption.9}{}}
\newlabel{sub@fig:100}{{c}{14}{$n=100$\relax }{figure.caption.9}{}}
\newlabel{fig:500}{{1d}{14}{$n=500$\relax }{figure.caption.9}{}}
\newlabel{sub@fig:500}{{d}{14}{$n=500$\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces $\left |\left |\beta ^{k}-\beta ^{k-1}\right |\right |_{\infty }$ for the well-conditioned $X$. This checks for convergence of the algorithm. It is clear that according to this measure, ISTA and FISTA perform the best for $n<p$, while ADMM performs the best for the $n>p$ case.\relax }}{14}{figure.caption.9}}
\newlabel{fig:cvgc}{{1}{14}{$\norm {\beta ^{k}-\beta ^{k-1}}_{\infty }$ for the well-conditioned $X$. This checks for convergence of the algorithm. It is clear that according to this measure, ISTA and FISTA perform the best for $n<p$, while ADMM performs the best for the $n>p$ case.\relax }{figure.caption.9}{}}
\newlabel{fig:20}{{2a}{15}{$n=20$\relax }{figure.caption.10}{}}
\newlabel{sub@fig:20}{{a}{15}{$n=20$\relax }{figure.caption.10}{}}
\newlabel{fig:50}{{2b}{15}{$n=50$\relax }{figure.caption.10}{}}
\newlabel{sub@fig:50}{{b}{15}{$n=50$\relax }{figure.caption.10}{}}
\newlabel{fig:100}{{2c}{15}{$n=100$\relax }{figure.caption.10}{}}
\newlabel{sub@fig:100}{{c}{15}{$n=100$\relax }{figure.caption.10}{}}
\newlabel{fig:500}{{2d}{15}{$n=500$\relax }{figure.caption.10}{}}
\newlabel{sub@fig:500}{{d}{15}{$n=500$\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Relative Normed Error from the true solution for the well conditioned $X$ matrix. This checks for convergence to the true solution. At $n=20$, the performance of ADMM is odd because it starts to rise after initially declining. This is not the case for all the other values of $n$, where it clearly outperforms the other methods.\relax }}{15}{figure.caption.10}}
\newlabel{fig:cvgc2}{{2}{15}{Relative Normed Error from the true solution for the well conditioned $X$ matrix. This checks for convergence to the true solution. At $n=20$, the performance of ADMM is odd because it starts to rise after initially declining. This is not the case for all the other values of $n$, where it clearly outperforms the other methods.\relax }{figure.caption.10}{}}
\newlabel{fig:20}{{3a}{16}{$n=20$\relax }{figure.caption.11}{}}
\newlabel{sub@fig:20}{{a}{16}{$n=20$\relax }{figure.caption.11}{}}
\newlabel{fig:50}{{3b}{16}{$n=50$\relax }{figure.caption.11}{}}
\newlabel{sub@fig:50}{{b}{16}{$n=50$\relax }{figure.caption.11}{}}
\newlabel{fig:100}{{3c}{16}{$n=100$\relax }{figure.caption.11}{}}
\newlabel{sub@fig:100}{{c}{16}{$n=100$\relax }{figure.caption.11}{}}
\newlabel{fig:500}{{3d}{16}{$n=500$\relax }{figure.caption.11}{}}
\newlabel{sub@fig:500}{{d}{16}{$n=500$\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Timing for Basis Pursuit for well-conditioned $X$. While it is clear that ADMM is the slowesty method, the timing for the other methods is all over the place. Hence it is difficult to find a clear winner in terms of the timing.\relax }}{16}{figure.caption.11}}
\newlabel{fig:timing}{{3}{16}{Timing for Basis Pursuit for well-conditioned $X$. While it is clear that ADMM is the slowesty method, the timing for the other methods is all over the place. Hence it is difficult to find a clear winner in terms of the timing.\relax }{figure.caption.11}{}}
\newlabel{fig:20}{{4a}{17}{$n=20$\relax }{figure.caption.12}{}}
\newlabel{sub@fig:20}{{a}{17}{$n=20$\relax }{figure.caption.12}{}}
\newlabel{fig:50}{{4b}{17}{$n=50$\relax }{figure.caption.12}{}}
\newlabel{sub@fig:50}{{b}{17}{$n=50$\relax }{figure.caption.12}{}}
\newlabel{fig:100}{{4c}{17}{$n=100$\relax }{figure.caption.12}{}}
\newlabel{sub@fig:100}{{c}{17}{$n=100$\relax }{figure.caption.12}{}}
\newlabel{fig:500}{{4d}{17}{$n=500$\relax }{figure.caption.12}{}}
\newlabel{sub@fig:500}{{d}{17}{$n=500$\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces $\left |\left |\beta ^{k}-\beta ^{k-1}\right |\right |_{\infty }$ for the ill-conditioned $X$. This checks for convergence of the algorithm. The sub-gradient algorithm diverges in this case and hence doesn't appear on the plots. It is clear that according to this measure, ISTA and FISTA performs erratically, but is still the best for $n<p$, while ADMM performs the best for the $n>p$ case.\relax }}{17}{figure.caption.12}}
\newlabel{fig:cvgccn}{{4}{17}{$\norm {\beta ^{k}-\beta ^{k-1}}_{\infty }$ for the ill-conditioned $X$. This checks for convergence of the algorithm. The sub-gradient algorithm diverges in this case and hence doesn't appear on the plots. It is clear that according to this measure, ISTA and FISTA performs erratically, but is still the best for $n<p$, while ADMM performs the best for the $n>p$ case.\relax }{figure.caption.12}{}}
\newlabel{fig:20}{{5a}{18}{$n=20$\relax }{figure.caption.13}{}}
\newlabel{sub@fig:20}{{a}{18}{$n=20$\relax }{figure.caption.13}{}}
\newlabel{fig:50}{{5b}{18}{$n=50$\relax }{figure.caption.13}{}}
\newlabel{sub@fig:50}{{b}{18}{$n=50$\relax }{figure.caption.13}{}}
\newlabel{fig:100}{{5c}{18}{$n=100$\relax }{figure.caption.13}{}}
\newlabel{sub@fig:100}{{c}{18}{$n=100$\relax }{figure.caption.13}{}}
\newlabel{fig:500}{{5d}{18}{$n=500$\relax }{figure.caption.13}{}}
\newlabel{sub@fig:500}{{d}{18}{$n=500$\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Relative Normed Error from the true solution for the ill-conditioned $X$ matrix. This checks for convergence to the true solution. ADMM clearly performs the best in this case.\relax }}{18}{figure.caption.13}}
\newlabel{fig:cvgc2cn}{{5}{18}{Relative Normed Error from the true solution for the ill-conditioned $X$ matrix. This checks for convergence to the true solution. ADMM clearly performs the best in this case.\relax }{figure.caption.13}{}}
\newlabel{fig:20}{{6a}{19}{$n=20$\relax }{figure.caption.14}{}}
\newlabel{sub@fig:20}{{a}{19}{$n=20$\relax }{figure.caption.14}{}}
\newlabel{fig:50}{{6b}{19}{$n=50$\relax }{figure.caption.14}{}}
\newlabel{sub@fig:50}{{b}{19}{$n=50$\relax }{figure.caption.14}{}}
\newlabel{fig:100}{{6c}{19}{$n=100$\relax }{figure.caption.14}{}}
\newlabel{sub@fig:100}{{c}{19}{$n=100$\relax }{figure.caption.14}{}}
\newlabel{fig:500}{{6d}{19}{$n=500$\relax }{figure.caption.14}{}}
\newlabel{sub@fig:500}{{d}{19}{$n=500$\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Timing for Basis Pursuit for ill-conditioned $X$. It is clear that ADMM is the slowesty method with the other methods performing much better. However, in terms of the other measures, their performance was poorer as compared to the ADMM.\relax }}{19}{figure.caption.14}}
\newlabel{fig:timingcn}{{6}{19}{Timing for Basis Pursuit for ill-conditioned $X$. It is clear that ADMM is the slowesty method with the other methods performing much better. However, in terms of the other measures, their performance was poorer as compared to the ADMM.\relax }{figure.caption.14}{}}
\bibcite{Beck}{{1}{}{{}}{{}}}
\bibcite{Boyd2011}{{2}{}{{}}{{}}}
\bibcite{Boyd}{{3}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
